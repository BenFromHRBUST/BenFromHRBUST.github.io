<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"bugmakerh.xyz","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文档为个人笔记整理。 原课程为发布于“中国大学MOOC”平台的由北京理工大学嵩天老师主讲的《Python 网络爬虫与信息提取》课程。故本文档引用了课程中部分源代码、图片等相关资源。  The Website is the API …">
<meta property="og:type" content="article">
<meta property="og:title" content="【笔记】Python网络爬虫与信息提取">
<meta property="og:url" content="http://bugmakerh.xyz/2022/02/04/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/index.html">
<meta property="og:site_name" content="BugMaker&#39;s Blog">
<meta property="og:description" content="本文档为个人笔记整理。 原课程为发布于“中国大学MOOC”平台的由北京理工大学嵩天老师主讲的《Python 网络爬虫与信息提取》课程。故本文档引用了课程中部分源代码、图片等相关资源。  The Website is the API …">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C0-0.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-4.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-9.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-10.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-11.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-12.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-1.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-2.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-3.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-5.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-6.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-7.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-8.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C1-13.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C2-1.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5CPIC%5C4-1.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C4-2.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C4-3.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C4-4.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C4-5.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C4-6.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C4-7.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C4-8.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C4-9.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C5-1.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C5-2.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C5-3.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C5-4.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C5-5.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C5-6.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C5-7.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C5-8.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C10-1.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C11-1.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C11-2.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C11-3.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//11-4.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//11-5.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//12-1.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//12-2.png">
<meta property="og:image" content="http://bugmakerh.xyz/.xyz//0-1.png">
<meta property="article:published_time" content="2022-02-04T13:54:00.000Z">
<meta property="article:modified_time" content="2022-08-27T07:59:41.008Z">
<meta property="article:author" content="BugMaker">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Regular Expression">
<meta property="article:tag" content="中国大学MOOC">
<meta property="article:tag" content="Open Course">
<meta property="article:tag" content="Network">
<meta property="article:tag" content="Crawler">
<meta property="article:tag" content="Requests">
<meta property="article:tag" content="Beautiful Soup">
<meta property="article:tag" content="Scrapy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://bugmakerh.xyz/.xyz//.%5Cpic%5C0-0.png">

<link rel="canonical" href="http://bugmakerh.xyz/2022/02/04/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>【笔记】Python网络爬虫与信息提取 | BugMaker's Blog</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?19d35097db41e3515c1116cf79269ba2";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">BugMaker's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Hello World!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-pasteme">

    <a href="http://pasteme.bugmakerh.xyz/" rel="noopener" target="_blank"><i class="fa fa-edit fa-fw"></i>PasteMe</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/BugMakerH" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://bugmakerh.xyz/2022/02/04/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="BugMaker">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BugMaker's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【笔记】Python网络爬虫与信息提取
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-04 13:54:00" itemprop="dateCreated datePublished" datetime="2022-02-04T13:54:00+00:00">2022-02-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-27 08:59:41" itemprop="dateModified" datetime="2022-08-27T08:59:41+01:00">2022-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Programming-Language/" itemprop="url" rel="index"><span itemprop="name">Programming Language</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文档为个人笔记整理。</p>
<p>原课程为发布于“中国大学MOOC”平台的由北京理工大学嵩天老师主讲的《<a target="_blank" rel="noopener" href="https://www.icourse163.org/course/BIT-1001870001">Python 网络爬虫与信息提取</a>》课程。故本文档引用了课程中部分源代码、图片等相关资源。</p>
<blockquote>
<p>The Website is the API …</p>
</blockquote>
<span id="more"></span>

<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><h2 id="课程简介"><a href="#课程简介" class="headerlink" title="课程简介"></a>课程简介</h2><p>《Python 网络爬虫与信息提取》的官方课程简介如下：</p>
<blockquote>
<p> 本课程面向具有 Python 编程基础的各类学习者，讲解利用 Python 语言爬取网络数据并提取关键信息的技术和方法，帮助学习者掌握定向网络数据爬取和网页解析的基本能力。</p>
<p>​    本课程介绍 Python 计算生态中最优秀的网络数据爬取和解析技术，具体讲授构建网络爬虫功能的两条重要技术路线：requests-bs4-re 和 Scrapy，所讲述内容广泛应用于 Amazon、Google、PayPal、Twitter 等国际知名公司。课程内容是进入大数据处理<strong>、</strong>数据挖掘、以数据为中心人工智能领域的必备实践基础。</p>
<p>​    本课程教学内容包括：</p>
<ul>
<li>Python 第三方库 Requests，讲解通过 HTTP&#x2F;HTTPS 协议自动从互联网获取数据并向其提交请求的方法；</li>
<li>Python 第三方库 Beautiful Soup，讲解从所爬取 HTML 页面中解析完整 Web 信息的方法；</li>
<li>Python 标准库 Re，讲解从所爬取 HTML 页面中提取关键信息的方法；</li>
<li>Python 第三方库 Scrapy，介绍通过网络爬虫框架构造专业网络爬虫的基本方法。</li>
</ul>
<p>​    本课程希望传递“理解和运用计算生态”的理念，重点培养学习者运用当代最优秀第三方专业资源，快速分析和解决问题的能力。”人生苦短，不要刀耕火种“，嵩老师教你直面问题和需求，用最好的工具解决它！</p>
<p>  本课程是国家精品在线开放课程“Python 网络爬虫与数据分析”课程的上半部分。“Python 网络爬虫与数据分析”课程由“Python 网络爬虫与信息提取”和“Python 数据分析与展示”两门MOOC 课程组成，完整地讲解了数据获取、清洗、统计、分析、可视化等数据处理周期的主要技术内容，培养计算思维、数据思维及采用程序设计方法解决计算问题的实战能力技术。 </p>
<p><img src="/.xyz//.%5Cpic%5C0-0.png" alt="0-0"></p>
</blockquote>
<p>《Python 网络爬虫与信息提取》的课程大纲如下：</p>
<blockquote>
<p>01</p>
<p>【第〇周】网络爬虫之前奏</p>
<p>课时</p>
<p>“网络爬虫”课程内容导学</p>
<p>Python语言开发工具选择</p>
<p>02</p>
<p>【第一周】网络爬虫之规则</p>
<p>课时</p>
<p>本周课程导学</p>
<p>单元1：Requests库入门</p>
<p>单元2：网络爬虫的“盗亦有道”</p>
<p>单元3：Requests库网络爬虫实战（5个实例）</p>
<p>03</p>
<p>【第二周】网络爬虫之提取</p>
<p>课时</p>
<p>本周课程导学</p>
<p>单元4：Beautiful Soup库入门</p>
<p>单元5：信息组织与提取方法</p>
<p>单元6：实例1：中国大学排名爬虫</p>
<p>04</p>
<p>【第三周】网络爬虫之实战</p>
<p>课时</p>
<p>本周课程导学</p>
<p>单元7：Re(正则表达式)库入门</p>
<p>单元8：实例2：淘宝商品比价定向爬虫</p>
<p>单元9：实例3：股票数据定向爬虫</p>
<p>05</p>
<p>【第四周】网络爬虫之框架</p>
<p>课时</p>
<p>本周课程导学</p>
<p>单元10：Scrapy爬虫框架</p>
<p>单元11：Scrapy爬虫基本使用</p>
<p>单元12：实例4：股票数据Scrapy爬虫</p>
</blockquote>
<h2 id="常用Python-IDE工具"><a href="#常用Python-IDE工具" class="headerlink" title="常用Python IDE工具"></a>常用Python IDE工具</h2><h3 id="文本类"><a href="#文本类" class="headerlink" title="文本类"></a>文本类</h3><ul>
<li>IDLE<ul>
<li>适用于 Python 入门</li>
<li>功能简单直接</li>
<li>300行代码以内</li>
</ul>
</li>
<li>Notepad++</li>
<li>Sublime Text<ul>
<li>专为程序员开发的第三方专用编程工具</li>
<li>专业编程体验</li>
<li>多种编程风格</li>
</ul>
</li>
<li>Vim &amp; Emacs</li>
<li>Atom</li>
<li>Komodo Edit</li>
</ul>
<h3 id="集成工具类IDE"><a href="#集成工具类IDE" class="headerlink" title="集成工具类IDE"></a>集成工具类IDE</h3><ul>
<li>PyCharm<ul>
<li>简单，集成度高</li>
<li>适合较复杂工程</li>
</ul>
</li>
<li>Wing<ul>
<li>公司维护，工具收费</li>
<li>调试功能丰富</li>
<li>版本控制，版本同步</li>
<li>适合多人共同开发</li>
<li>适用于上千行或者上万行的大型项目</li>
</ul>
</li>
<li>PyDev &amp; Eclipse<ul>
<li>开源 IDE 开发工具</li>
<li>需要有一定开发经验</li>
</ul>
</li>
<li>Visual Studio &amp; PTVS<ul>
<li>微软公司维护</li>
<li>Win 环境为主</li>
<li>调试功能丰富</li>
</ul>
</li>
<li>Anaconda &amp; Spyder<ul>
<li>开源免费</li>
<li>支持近 800 个第三方库</li>
</ul>
</li>
<li>Canopy<ul>
<li>公司维护，工具收费</li>
<li>支持近 500 个第三方库</li>
<li>适合科学计算领域应用开发</li>
</ul>
</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>Python Requests: <a target="_blank" rel="noopener" href="https://www.python-requests.org/">https://www.python-requests.org</a></li>
<li>Python Beautiful Soup: <a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup">https://www.crummy.com/software/BeautifulSoup</a></li>
<li>Python Scrapy: <a target="_blank" rel="noopener" href="https://scrapy.org/">https://scrapy.org/</a></li>
</ol>
<h1 id="Requests库"><a href="#Requests库" class="headerlink" title="Requests库"></a>Requests库</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Requests 库的主要方法如下：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>requests.request()</td>
<td>构造一个请求，支撑以下个方法的基础</td>
</tr>
<tr>
<td>requests.get()</td>
<td>获取 HTML 网页的主要方法，对应于 HTTP 的 GET</td>
</tr>
<tr>
<td>requests.head()</td>
<td>获取 HTML 网页头信息的方法，对应于 HTTP 的 HEAD</td>
</tr>
<tr>
<td>requests.post()</td>
<td>向 HTML 网页提交 POST 请求的方法，对应于 HTTP 的 POST</td>
</tr>
<tr>
<td>requests.put()</td>
<td>向 HTML 网页提交 PUT 请求的方法，对应于 HTTP 的 PUT</td>
</tr>
<tr>
<td>requests.patch()</td>
<td>向 HTML 网页提交局部修改请求，对应于 HTTP 的 PATCH</td>
</tr>
<tr>
<td>requests.delete()</td>
<td>向 HTML 网页提交删除请求，对应于 HTTP 的 DELETE</td>
</tr>
</tbody></table>
<h2 id="HTTP-协议"><a href="#HTTP-协议" class="headerlink" title="HTTP 协议"></a>HTTP 协议</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>HTTP，全称 Hypertext Transfer Protocol，即超文本传输协议。它是一种基于“请求与响应”模式的、无状态的应用层协议。HTTP 协议采用 URL 作为定位网络资源的标识。</p>
<h3 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h3><p>URL 格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://host[:post][path]</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li>host：合法的 Internet 主机域名或 IP 地址</li>
<li>port：端口号，缺省端口为 80</li>
<li>path：请求资源的路径</li>
</ul>
<p>URL 是通过 HTTP 协议存取资源的 Internet 路径，一个 URL 对应一个数据资源。</p>
<h3 id="对资源的操作"><a href="#对资源的操作" class="headerlink" title="对资源的操作"></a>对资源的操作</h3><p>HTTP 协议对资源的操作主要有以下功能：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>GET</td>
<td>请求获取 URL 位置的资源</td>
</tr>
<tr>
<td>HEAD</td>
<td>请求获取 URL 位置资源的响应消息报告，即获得该资源的头部信息</td>
</tr>
<tr>
<td>POST</td>
<td>请求向 URL 位置的资源后附加新的数据</td>
</tr>
<tr>
<td>PUT</td>
<td>请求向 URL 位置存储一个资源，覆盖原 URL 位置的资源</td>
</tr>
<tr>
<td>PATCH</td>
<td>请求局部更新 URL 位置的资源，即改变</td>
</tr>
<tr>
<td>DELETE</td>
<td>请求删除 URL 位置存储的资源</td>
</tr>
</tbody></table>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p>HTTP 协议操作流程，如下图所示：</p>
<p><img src="/.xyz//.%5Cpic%5C1-4.png" alt="1-4"></p>
<h3 id="PATCH-与-PUT-的区别"><a href="#PATCH-与-PUT-的区别" class="headerlink" title="PATCH 与 PUT 的区别"></a>PATCH 与 PUT 的区别</h3><p>有如下例子：</p>
<blockquote>
<p>假设 URL 位置有一组数据 UserInfo，包括 UserID、UserName 等20个字段。<br>需求：用户修改了 UserName，其他不变。<br>采用 PATCH 和 PUT 的区别如下：</p>
</blockquote>
<ul>
<li>采用PATCH，仅向URL提交UserNamel的局部更新请求。</li>
<li>采用PUT，必须将所有20个字段一井提交到URL，未提交字段被删除。</li>
</ul>
<p>PATCH的最主要好处：节省网络带宽。</p>
<h2 id="库"><a href="#库" class="headerlink" title="库"></a>库</h2><h3 id="request"><a href="#request" class="headerlink" title="request( )"></a>request( )</h3><p>request() 方法是 requests 库最基本的方法。</p>
<p><code>request()</code> 方法的使用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.request(method, url, **kwargs)</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li><p>method：请求方式，对应 get&#x2F;put&#x2F;post 等 7 种，如下：</p>
<ul>
<li><code>r = requests.request(&quot;GET&quot;, url, **kwargs)</code></li>
<li><code>r = requests.request(&quot;HEAD&quot;, url, **kwargs)</code></li>
<li><code>r = requests.request(&quot;POST&quot;, url, **kwargs)</code></li>
<li><code>r = requests.request(&quot;PUT&quot;, url, **kwargs)</code></li>
<li><code>r = requests.request(&quot;PATCH&quot;, url, **kwargs)</code></li>
<li><code>r = requests.request(&quot;delete&quot;, url, **kwargs)</code></li>
<li><code>r = requests.request(&quot;OPTIONS&quot;, url, **kwargs)</code></li>
</ul>
</li>
<li><p>url：拟获取页面的 url 链接</p>
</li>
<li><p>**kwargs：控制访问参数，共 13 个</p>
<ul>
<li><p>params：字典或字节序列，作为参数增加到 url 中</p>
<p><img src="/.xyz//.%5Cpic%5C1-9.png" alt="1-9"></p>
</li>
<li><p>data：字典、字节序列或文件对象，作为 Request 的内容</p>
<p><img src="/.xyz//.%5Cpic%5C1-10.png" alt="1-10"></p>
</li>
<li><p>json：JSON 格式的数据，作为 Request 的内容</p>
<p><img src="/.xyz//.%5Cpic%5C1-11.png" alt="1-11"></p>
</li>
<li><p>headers：字典，HTTP 定制头</p>
<p><img src="/.xyz//.%5Cpic%5C1-12.png" alt="1-12"></p>
</li>
<li><p>cookies：字典或 CookieJar，Request 中的 cookie</p>
</li>
<li><p>auth：元组，支持 HTTP 认证功能</p>
</li>
<li><p>files：字典类型，传输文件</p>
</li>
<li><p>timeout：设定超时时间，秒为单位</p>
</li>
<li><p>proxies：字典类型，设定访问代理服务器，可以增加登录认证</p>
</li>
<li><p>allow_redirects：True&#x2F;False，默认为 True，重定向开关</p>
</li>
<li><p>stream：True&#x2F;False，默认为 True，获取内容立即下载开关</p>
</li>
<li><p>verify：True&#x2F;False，默认为 True，认证 SSL 证书开关</p>
</li>
<li><p>cert：本地 SSL 证书路径</p>
</li>
</ul>
</li>
</ul>
<h3 id="get"><a href="#get" class="headerlink" title="get( )"></a>get( )</h3><h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><p><code>get()</code> 方法的使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r = requests.get(url, params=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li>url：拟获取页面的 url 链接</li>
<li>params：url 中的额外参数，字典或字节流格式，可选</li>
<li>**kwargs：12 个控制访问的参数（request() 中除 params 的 12 个），可选</li>
</ul>
<p><code>get()</code> 方法将会：</p>
<ul>
<li>构造一个向服务器请求资源的 Request 对象</li>
<li>返回一个包含服务器资源的 Response 对象（Response 对象包含爬虫返回的内容）</li>
</ul>
<p>实例代码如下：</p>
<p><img src="/.xyz//.%5Cpic%5C1-1.png" alt="1-1"></p>
<p>例中：</p>
<ol>
<li>使用 <code>get()</code> 访问百度主页</li>
<li>使用 <code>r.status_code</code> 检测请求的状态码（状态码为 200，则访问成功）</li>
<li>使用 <code>type()</code> 检测 <code>get()</code> 返回值的类型（Response 类型）</li>
<li>使用 <code>r.headers</code> 返回 <code>GET</code> 请求获得页面的头部信息</li>
</ol>
<h4 id="Response-对象"><a href="#Response-对象" class="headerlink" title="Response 对象"></a>Response 对象</h4><p>Response 对象的常用属性如下：</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r.status_code</td>
<td>HTTP 请求的返回状态，200 表示连接成功，404 表示失败</td>
</tr>
<tr>
<td>r.text</td>
<td>HTTP 响应内容的字符串形式，即 url 对应的页面内容</td>
</tr>
<tr>
<td>r.encoding</td>
<td>从 HTTP header 中猜测的响应内容编码方式</td>
</tr>
<tr>
<td>r.apparent_encoding</td>
<td>从内容中分析出的响应内容编码方式（备选编码方式）</td>
</tr>
<tr>
<td>r.content</td>
<td>HTTP 响应内容的二进制形式</td>
</tr>
</tbody></table>
<p>对于 encoding 属性，如果 header 中不存在 charset，则认为编码为 ISO-8859-1。</p>
<p>使用流程如下：</p>
<p><img src="/.xyz//.%5Cpic%5C1-2.png" alt="1-2"></p>
<p>实例代码如下：</p>
<p><img src="/.xyz//.%5Cpic%5C1-3.png" alt="1-3"></p>
<p>其中：</p>
<ol>
<li>使用 <code>r.text</code> 查看内容时发现有部分乱码</li>
<li>通过 <code>r.encoding</code> 查看编码格式为“ISO-8859-1”</li>
<li>通过 <code>r.apparent_encoding</code> 分析出备选编码方式为“utf-8”</li>
<li>修改 <code>r.encoding</code> 为“utf-8”</li>
<li>再次使用 <code>r.text</code> 查看内容，显示正常</li>
</ol>
<h3 id="head"><a href="#head" class="headerlink" title="head( )"></a>head( )</h3><p><code>head()</code> 方法的使用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.head(url, **kwargs)</span><br></pre></td></tr></table></figure>

<p>示例代码如下：</p>
<p><img src="/.xyz//.%5Cpic%5C1-5.png" alt="1-5"></p>
<h3 id="post"><a href="#post" class="headerlink" title="post( )"></a>post( )</h3><p><code>post()</code> 方法的使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.post(url, data=<span class="literal">None</span>, json=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure>

<p>示例代码1如下：</p>
<p><img src="/.xyz//.%5Cpic%5C1-6.png" alt="1-6"></p>
<p>上述示例代码1向 URL POST 一个字典，自动编码为 form（表单）。</p>
<p>示例代码2如下：</p>
<p><img src="/.xyz//.%5Cpic%5C1-7.png" alt="1-7"></p>
<p>上述示例代码2向 URL POST 一个字符串，自动编码为 data。</p>
<h3 id="put"><a href="#put" class="headerlink" title="put( )"></a>put( )</h3><p><code>put()</code> 方法的使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.patch(url, data=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure>

<p>示例代码如下：</p>
<p><img src="/.xyz//.%5Cpic%5C1-8.png" alt="1-8"></p>
<h3 id="delete"><a href="#delete" class="headerlink" title="delete( )"></a>delete( )</h3><p><code>delete()</code> 方法的使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.delete(url, **kwargs)</span><br></pre></td></tr></table></figure>

<h3 id="r-request-headers"><a href="#r-request-headers" class="headerlink" title="r.request.headers"></a>r.request.headers</h3><p>用于返回 HTTP 请求的头部信息。</p>
<p>示例如下：</p>
<p><img src="/.xyz//.%5Cpic%5C1-13.png" alt="1-13"></p>
<p>修改头部信息的方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kv = &#123;<span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0&#x27;</span>&#125;  <span class="comment"># 构造键值对</span></span><br><span class="line">r = requests.get(url, headers = kv)</span><br></pre></td></tr></table></figure>

<h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p>Requests 库支持的常用连接异常如下：</p>
<table>
<thead>
<tr>
<th>异常</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>requests.ConnectionError</td>
<td>网络连接错误异常，如 DNS 查询失败、拒绝连接等</td>
</tr>
<tr>
<td>requests.HTTPError</td>
<td>HTTP 错误异常</td>
</tr>
<tr>
<td>requests.URLRequired</td>
<td>URL 缺失异常</td>
</tr>
<tr>
<td>requests.TooManyRedirects</td>
<td>超过最大重定向次数，产生重定向异常</td>
</tr>
<tr>
<td>requests.ConnectTimeout</td>
<td>连接远程服务器超时异常</td>
</tr>
<tr>
<td>requests.Timeout</td>
<td>请求 URL 超时，产生超时异常</td>
</tr>
</tbody></table>
<p>Response 提供的异常方法：</p>
<table>
<thead>
<tr>
<th>异常</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r.raise_for_status()</td>
<td>如果不是 200，产生异常 requests.HTTPError</td>
</tr>
</tbody></table>
<h2 id="图片的爬取"><a href="#图片的爬取" class="headerlink" title="图片的爬取"></a>图片的爬取</h2><p>网络图片链接格式：<code>http://www.example.com/picture.jpg</code>。</p>
<h2 id="爬取网页的通用代码框架"><a href="#爬取网页的通用代码框架" class="headerlink" title="爬取网页的通用代码框架"></a>爬取网页的通用代码框架</h2><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getHTMLText</span>(<span class="params">url</span>):</span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">		r.raise_for_status()    <span class="comment"># 如果状态不是200，引发HTTPError异常</span></span><br><span class="line">		r.encoding = r.apparent_encoding</span><br><span class="line">		<span class="keyword">return</span> r.text</span><br><span class="line">	<span class="keyword">except</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;产生异常&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	url = <span class="string">&quot;http://www.baidu.com&quot;</span></span><br><span class="line">	<span class="built_in">print</span>(getHTMLText(url))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><h3 id="实例1：京东商品页面的爬取"><a href="#实例1：京东商品页面的爬取" class="headerlink" title="实例1：京东商品页面的爬取"></a>实例1：京东商品页面的爬取</h3><p>爬取信息的网页为：<a target="_blank" rel="noopener" href="https://item.jd.com/2967929.html">https://item.jd.com/2967929.html</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&quot;https://item.jd.com/2967929.html&quot;</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	r = requests.get(url)</span><br><span class="line">	r.raise_for_status()</span><br><span class="line">	r.encoding = r.apparent_encoding</span><br><span class="line">	<span class="built_in">print</span>(r.text[:<span class="number">1000</span>])</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;爬取失败&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="实例2：亚马逊商品页面的爬取"><a href="#实例2：亚马逊商品页面的爬取" class="headerlink" title="实例2：亚马逊商品页面的爬取"></a>实例2：亚马逊商品页面的爬取</h3><p>爬取信息的网页为：<a target="_blank" rel="noopener" href="https://www.amazon.cn/gp/product/B01M8L5Z3Y">https://www.amazon.cn/gp/product/B01M8L5Z3Y</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&quot;https://www.amazon.cn/gp/product/B01M8L5Z3Y&quot;</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	kv = &#123;<span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0&#x27;</span>&#125;  <span class="comment"># 构造键值对</span></span><br><span class="line">	r = requests.get(url, headers=kv)</span><br><span class="line">	r.raise_for_status()</span><br><span class="line">	r.encoding = r.apparent_encoding</span><br><span class="line">	<span class="built_in">print</span>(r.text[<span class="number">1000</span>:<span class="number">2000</span>])</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;爬取失败&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="实例3：百度360搜索关键词提交"><a href="#实例3：百度360搜索关键词提交" class="headerlink" title="实例3：百度360搜索关键词提交"></a>实例3：百度360搜索关键词提交</h3><p>百度的关键词接口：<code>http://www.baidu.com/s?wd=[keyword]</code>。</p>
<p>360 的关键词接口：<code>http://www.so.com/s?q=[keyword]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">keyword = <span class="string">&quot;Python&quot;</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	kv = &#123;<span class="string">&#x27;wd&#x27;</span>:keyword&#125;</span><br><span class="line">	r = requests.get(<span class="string">&quot;http://www.baidu.com/s&quot;</span>, params=kv)</span><br><span class="line">	<span class="built_in">print</span>(r.request.url)</span><br><span class="line">	r.raise_for_status()</span><br><span class="line">	<span class="built_in">print</span>(<span class="built_in">len</span>(r.text))</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;爬取失败&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="实例4：网络图片的爬取与存储"><a href="#实例4：网络图片的爬取与存储" class="headerlink" title="实例4：网络图片的爬取与存储"></a>实例4：网络图片的爬取与存储</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">url = <span class="string">&quot;http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg&quot;</span></span><br><span class="line">root = <span class="string">&quot;.//pic//&quot;</span></span><br><span class="line">path = root + url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(root):</span><br><span class="line">		os.mkdir(root)</span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">		r = requests.get(url)</span><br><span class="line">		<span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">			f.write(r.content)</span><br><span class="line">			f.close()</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&quot;文件保存成功&quot;</span>)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;文件已存在&quot;</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;爬取失败&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="实例5：IP地址归属地的自动查询"><a href="#实例5：IP地址归属地的自动查询" class="headerlink" title="实例5：IP地址归属地的自动查询"></a>实例5：IP地址归属地的自动查询</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&quot;https://m.ip138.com/iplookup.asp?ip=?ip=&quot;</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	r = requests.get(url+<span class="string">&#x27;202.204.80.112&#x27;</span>)</span><br><span class="line">	r.raise_for_status()</span><br><span class="line">	r.encoding = r.apparent_encoding</span><br><span class="line">	<span class="built_in">print</span>(r.text[-<span class="number">500</span>:])</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">	<span class="built_in">print</span>(e)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;爬取失败&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="网络爬虫的问题"><a href="#网络爬虫的问题" class="headerlink" title="网络爬虫的问题"></a>网络爬虫的问题</h1><h2 id="网络爬虫的尺寸"><a href="#网络爬虫的尺寸" class="headerlink" title="网络爬虫的尺寸"></a>网络爬虫的尺寸</h2><p><img src="/.xyz//.%5Cpic%5C2-1.png" alt="2-1"></p>
<h2 id="网络爬虫的限制"><a href="#网络爬虫的限制" class="headerlink" title="网络爬虫的限制"></a>网络爬虫的限制</h2><p>常用的限制方式如下：</p>
<ul>
<li>来源审查：判断 User-Agent 进行限制<ul>
<li>检查来访 HTTP 协议头的 User-Agent 域，只响应浏览器或友好爬虫的访问。</li>
</ul>
</li>
<li>发布公告：Robots 协议<ul>
<li>告知所有爬虫网站的爬取策略，要求爬虫遵守。</li>
</ul>
</li>
</ul>
<h2 id="Robots-协议"><a href="#Robots-协议" class="headerlink" title="Robots 协议"></a>Robots 协议</h2><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><p>Robots 协议，全称 Robots Exclusion Standard，即网络爬虫排除标准。</p>
<p>作用：网站告知网络爬虫哪些页面可以抓取，哪些不行。</p>
<p>形式：在网站的根目录下放置 robots.txt 文件。</p>
<h3 id="遵守方式"><a href="#遵守方式" class="headerlink" title="遵守方式"></a>遵守方式</h3><p>网络爬虫：自动或人工识别 robots.txt，再进行内容爬取。</p>
<p>约束性：Robots 协议是建议但非约束性，网络爬虫可以不遵守，但存在法律风险。</p>
<h1 id="BeautifulSoup库"><a href="#BeautifulSoup库" class="headerlink" title="BeautifulSoup库"></a>BeautifulSoup库</h1><h2 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h2><p>Beautiful Soup 库是解析、遍历、维护“标签树”的功能库。</p>
<p>常用的引入方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bs4</span><br></pre></td></tr></table></figure>

<h2 id="解析器"><a href="#解析器" class="headerlink" title="解析器"></a>解析器</h2><table>
<thead>
<tr>
<th>解析器</th>
<th>使用方法</th>
<th>条件</th>
</tr>
</thead>
<tbody><tr>
<td>bs4的HTML解析器</td>
<td>BeautifulSoup(mk, ‘html.parser’)</td>
<td>安装bs4库</td>
</tr>
<tr>
<td>lxml的HTML解析器</td>
<td>BeautifulSoup(mk, ‘lxml’)</td>
<td>pip install lxml</td>
</tr>
<tr>
<td>lxml的XML解析器</td>
<td>BeautifulSoup(mk, ‘xml’)</td>
<td>pip install lxml</td>
</tr>
<tr>
<td>html5lib的解析器</td>
<td>BeautifulSoup(mk, ‘html5lib’)</td>
<td>pip install html5lib</td>
</tr>
</tbody></table>
<h2 id="基本元素"><a href="#基本元素" class="headerlink" title="基本元素"></a>基本元素</h2><table>
<thead>
<tr>
<th>基本元素</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Tag</td>
<td>标签，最基本的信息组织单元，分别用&lt;&gt;和&lt;&#x2F;&gt;标明开头和结尾</td>
</tr>
<tr>
<td>Name</td>
<td>标签的名字，&lt;p&gt;…&lt;&#x2F;p&gt;的名字是’p‘，格式：<code>&lt;tag&gt;.name</code></td>
</tr>
<tr>
<td>Attributes</td>
<td>标签的属性，字典形式组织，格式：<code>&lt;tag&gt;.attrs</code></td>
</tr>
<tr>
<td>NavigableString</td>
<td>标签内非属性字符串，&lt;&gt;…&lt;&#x2F;&gt;中字符串，格式：<code>&lt;tag&gt;.string</code></td>
</tr>
<tr>
<td>Comment</td>
<td>标签内字符串的注释部分，一种特殊的Comment类型</td>
</tr>
</tbody></table>
<p>示例如下：</p>
<p><img src="/.xyz//.%5CPIC%5C4-1.png" alt="4-1"></p>
<p><img src="/.xyz//.%5Cpic%5C4-2.png" alt="4-2"></p>
<h2 id="遍历方法"><a href="#遍历方法" class="headerlink" title="遍历方法"></a>遍历方法</h2><h3 id="下行遍历"><a href="#下行遍历" class="headerlink" title="下行遍历"></a>下行遍历</h3><table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.contents</td>
<td>子节点的列表，将&lt;tag&gt;所有儿子节点存入列表</td>
</tr>
<tr>
<td>.children</td>
<td>子节点的迭代类型，与.contents类似，用于循环遍历儿子节点</td>
</tr>
<tr>
<td>.descendants</td>
<td>子孙节点的迭代类型，包含所有子孙节点，用于循环遍历</td>
</tr>
</tbody></table>
<p><img src="/.xyz//.%5Cpic%5C4-3.png" alt="4-3"></p>
<h3 id="上行遍历"><a href="#上行遍历" class="headerlink" title="上行遍历"></a>上行遍历</h3><table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.parent</td>
<td>节点的父亲标签</td>
</tr>
<tr>
<td>.parents</td>
<td>节点先辈标签的迭代类型，用于循环遍历先辈节点</td>
</tr>
</tbody></table>
<p><img src="/.xyz//.%5Cpic%5C4-4.png" alt="4-4"></p>
<h3 id="平行遍历"><a href="#平行遍历" class="headerlink" title="平行遍历"></a>平行遍历</h3><table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.next_sibling</td>
<td>返回按照HTML文本顺序的下一个平行节点标签</td>
</tr>
<tr>
<td>.previous_sibling</td>
<td>返回按照HTML文本顺序的上一个平行节点标签</td>
</tr>
<tr>
<td>.next_siblings</td>
<td>迭代类型，返回按照HTML文本顺序的后续所有平行节点标签</td>
</tr>
<tr>
<td>.previous_siblings</td>
<td>迭代类型，返回按照HTML文本顺序的前续所有平行节点标签</td>
</tr>
</tbody></table>
<p>注：平行遍历发生再同一个父节点下的各节点间。</p>
<h2 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h2><p>使用 prettify() 函数。</p>
<p><img src="/.xyz//.%5Cpic%5C4-5.png" alt="4-5"></p>
<h2 id="内容查找"><a href="#内容查找" class="headerlink" title="内容查找"></a>内容查找</h2><h3 id="find-all"><a href="#find-all" class="headerlink" title="find_all"></a>find_all</h3><h4 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h4><p>使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;&gt;.find_all(name, attrs, recursive, string, **kwargs)</span><br></pre></td></tr></table></figure>

<p>返回一个列表类型，存储查找的结果。</p>
<h4 id="name"><a href="#name" class="headerlink" title="name"></a>name</h4><p>对标签名称的检索字符串。</p>
<p><img src="/.xyz//.%5Cpic%5C4-6.png" alt="4-6"></p>
<h4 id="attrs"><a href="#attrs" class="headerlink" title="attrs"></a>attrs</h4><p>对标签属性值的检索字符串，可标注属性检索。</p>
<p><img src="/.xyz//.%5Cpic%5C4-7.png" alt="4-7"></p>
<h4 id="recursive"><a href="#recursive" class="headerlink" title="recursive"></a>recursive</h4><p>是否对子孙全部检索，默认 True。</p>
<p><img src="/.xyz//.%5Cpic%5C4-8.png" alt="4-8"></p>
<h4 id="string"><a href="#string" class="headerlink" title="string"></a>string</h4><p>&lt;&gt;…&lt;&#x2F;&gt; 中字符串区域的检索字符串。</p>
<p><img src="/.xyz//.%5Cpic%5C4-9.png" alt="4-9"></p>
<p><code>&lt;tag&gt;(..)</code> 等价于 <code>&lt;tag&gt;.find_all(..)</code>。</p>
<h3 id="扩展方法"><a href="#扩展方法" class="headerlink" title="扩展方法"></a>扩展方法</h3><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>&lt;&gt;.find()</td>
<td>搜索且只返回一个结果，字符串类型，同 .find_all() 参数</td>
</tr>
<tr>
<td>&lt;&gt;.find_parents()</td>
<td>在先辈节点中搜索，返回列表类型，同 .find_all() 参数</td>
</tr>
<tr>
<td>&lt;&gt;.find_parent()</td>
<td>在先辈节点中返回一个结果，字符串类型，同 .find_all() 参数</td>
</tr>
<tr>
<td>&lt;&gt;.find_next_siblings()</td>
<td>在后续平行节点中搜索，返回列表类型，同 .find_all() 参数</td>
</tr>
<tr>
<td>&lt;&gt;.find_nex_sibling()</td>
<td>在后续平行节点中返回一个结果，字符串类型，同 .find_all() 参数</td>
</tr>
<tr>
<td>&lt;&gt;.find_previous_siblings()</td>
<td>在前序平行节点中搜素，返回列表类型，同 .find_all() 参数</td>
</tr>
<tr>
<td>&lt;&gt;.find_previous_sibling()</td>
<td>在前序平行节点中返回一个结果，字符串类型，同 .find_all() 参数</td>
</tr>
</tbody></table>
<h1 id="实例1：中国大学排名定向爬虫"><a href="#实例1：中国大学排名定向爬虫" class="headerlink" title="实例1：中国大学排名定向爬虫"></a>实例1：中国大学排名定向爬虫</h1><h2 id="程序的结构设计"><a href="#程序的结构设计" class="headerlink" title="程序的结构设计"></a>程序的结构设计</h2><ol>
<li>从网络上获取大学排名网页内容（<code>getHTMLText()</code>）</li>
<li>提取网页内容中信息到合适的数据结构（<code>fillUnivList()</code>）</li>
<li>利用数据结构展示并输出结果（<code>printUnivList()</code>）</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getHTMLText</span>(<span class="params">url</span>):</span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">		r.raise_for_status()</span><br><span class="line">		r.encoding = r.apparent_encoding</span><br><span class="line">		<span class="keyword">return</span> r.text</span><br><span class="line">	<span class="keyword">except</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fillUnivList</span>(<span class="params">ulist, html</span>):</span><br><span class="line">	soup = BeautifulSoup(html, <span class="string">&quot;html.parser&quot;</span>)</span><br><span class="line">	<span class="keyword">for</span> tr <span class="keyword">in</span> soup.find(<span class="string">&#x27;tbody&#x27;</span>).children:</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">isinstance</span>(tr, bs4.element.Tag):</span><br><span class="line">			tds = tr(<span class="string">&#x27;td&#x27;</span>)</span><br><span class="line">			ulist.append([tds[<span class="number">0</span>].string.strip(), tds[<span class="number">1</span>](<span class="string">&#x27;a&#x27;</span>)[<span class="number">0</span>].string.strip(), tds[<span class="number">5</span>].string.strip()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">printUnivList</span>(<span class="params">ulist, num</span>):</span><br><span class="line">	tplt = <span class="string">&quot;&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;&quot;</span></span><br><span class="line">	<span class="built_in">print</span>(tplt.<span class="built_in">format</span>(<span class="string">&quot;排名&quot;</span>, <span class="string">&quot;学校名称&quot;</span>, <span class="string">&quot;总分&quot;</span>, <span class="built_in">chr</span>(<span class="number">12288</span>)))</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">		u = ulist[i]</span><br><span class="line">		<span class="built_in">print</span>(tplt.<span class="built_in">format</span>(u[<span class="number">0</span>], u[<span class="number">1</span>], u[<span class="number">2</span>], <span class="built_in">chr</span>(<span class="number">12288</span>)))</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Suc&quot;</span>, <span class="built_in">str</span>(num))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">	uinfo = []</span><br><span class="line">	url = <span class="string">&quot;https://www.shanghairanking.cn/rankings/bcur/2021&quot;</span></span><br><span class="line">	html = getHTMLText(url)</span><br><span class="line">	fillUnivList(uinfo, html)</span><br><span class="line">	printUnivList(uinfo, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Re库"><a href="#Re库" class="headerlink" title="Re库"></a>Re库</h1><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><h3 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h3><ul>
<li>通用的字符串表达框架</li>
<li>简洁表达一组字符串的表达式</li>
<li>针对字符表达“简洁”和“特征”思想的工具</li>
<li>判断某字符串的特征归属</li>
</ul>
<h3 id="常见用途"><a href="#常见用途" class="headerlink" title="常见用途"></a>常见用途</h3><ul>
<li>表达文本类型的特征（病毒、入侵等）</li>
<li>同时查找或替换一组字符串</li>
<li>匹配字符串的全部或部分</li>
</ul>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ul>
<li>编译：将符合正则表达式语法的字符串转换成正则表达式特征。</li>
</ul>
<h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><h4 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h4><p>正则表达式是由字符和操作符组成的。</p>
<h4 id="常用操作符"><a href="#常用操作符" class="headerlink" title="常用操作符"></a>常用操作符</h4><table>
<thead>
<tr>
<th>操作符</th>
<th>说明</th>
<th>实例</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td>表示任何单个字符</td>
<td></td>
</tr>
<tr>
<td>[ ]</td>
<td>字符集，对单个字符给出取值范围</td>
<td>[abc]表示a、b、c，[a-z]表示a到z单个字符</td>
</tr>
<tr>
<td>[^ ]</td>
<td>非字符集，对单个字符给出排除范围</td>
<td>[^abc]表示非a或b或c的单个字符</td>
</tr>
<tr>
<td>*</td>
<td>前一个字符0次或无限次扩展</td>
<td>abc*表示ab、abc、abcc、abccc等</td>
</tr>
<tr>
<td>+</td>
<td>前一个字符1次或无限次扩展</td>
<td>abc+表示abc、abcc、abccc等</td>
</tr>
<tr>
<td>?</td>
<td>前一个字符0次或1次扩展</td>
<td>abc?表示ab、abc</td>
</tr>
<tr>
<td>|</td>
<td>左右表达式任意一个</td>
<td>abc|def表示abc、def</td>
</tr>
<tr>
<td>{m}</td>
<td>扩展前一个字符m次</td>
<td>ab{2}c表示abbc</td>
</tr>
<tr>
<td>{m,n}</td>
<td>扩展前一个字符m至n次（含n）</td>
<td>ab{1,2}c表示abc、abbc</td>
</tr>
<tr>
<td>^</td>
<td>匹配字符串开头</td>
<td>^abc表示abc且在一个字符串的开头</td>
</tr>
<tr>
<td>$</td>
<td>匹配字符串结尾</td>
<td>abc$表示abc且在一个字符串的结尾</td>
</tr>
<tr>
<td>( )</td>
<td>分组标记，内部只能使用 | 操作符</td>
<td>(abc)表示abc，(abc|def)表示abc、def</td>
</tr>
<tr>
<td>\d</td>
<td>数字，等价于[0-9]</td>
<td></td>
</tr>
<tr>
<td>\w</td>
<td>单词字符，等价于[A-Za-z0-9_]</td>
<td></td>
</tr>
</tbody></table>
<h4 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h4><table>
<thead>
<tr>
<th>正则表达式</th>
<th>对应字符串</th>
</tr>
</thead>
<tbody><tr>
<td>P(Y|YT|YTH|YTHO)?N</td>
<td>PY、PYN、PYTN、PYTHN、PYTHON</td>
</tr>
<tr>
<td>PYTHON+</td>
<td>PYTHON、PYTHONN、PYTHONNN…</td>
</tr>
<tr>
<td>PY[TH]ON</td>
<td>PYTON、PYHON</td>
</tr>
<tr>
<td>PY[^TH]?ON</td>
<td>PYON、PYaON、PYbON、PYcON…</td>
</tr>
<tr>
<td>PY{:3}N</td>
<td>PN、PYN、PYYN、PYYYN</td>
</tr>
</tbody></table>
<h2 id="Re库概述"><a href="#Re库概述" class="headerlink" title="Re库概述"></a>Re库概述</h2><p>Re 库是 Python 的标准库，主要用于字符串匹配。</p>
<p>调用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure>

<p>re 库通常使用 raw string 类型（原生字符串类型，即不包含转义符的字符串）表示正则表达式，表示为 <code>r&#39;text&#39;</code>。</p>
<h2 id="主要功能函数"><a href="#主要功能函数" class="headerlink" title="主要功能函数"></a>主要功能函数</h2><h3 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h3><table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>re.search()</td>
<td>在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象</td>
</tr>
<tr>
<td>re.match()</td>
<td>从一个字符串的开始位置起匹配正则表达式，返回match对象</td>
</tr>
<tr>
<td>re.findall()</td>
<td>搜索字符串，以列表类型返回全部能匹配的子串</td>
</tr>
<tr>
<td>re.split()</td>
<td>将一个字符串按照正则表达式匹配结果进行分割，返回列表类型</td>
</tr>
<tr>
<td>re.finditer()</td>
<td>搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象</td>
</tr>
<tr>
<td>re.sub()</td>
<td>在一个字符串中替换所有匹配正则表达式的子串，返回替换后的子串</td>
</tr>
</tbody></table>
<h3 id="re-search"><a href="#re-search" class="headerlink" title="re.search()"></a>re.search()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.search(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象</p>
<ul>
<li><p>pattern：正则表达式的字符串或原生字符串表示</p>
</li>
<li><p>string：待匹配字符串</p>
</li>
<li><p>flags：正则表达式使用时的控制标记，常用标记如下：</p>
<table>
<thead>
<tr>
<th>常用标记</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>re.I re.IGNORECASE</td>
<td>忽略正则表达式的大小写，[A-Z]能够匹配小写字符</td>
</tr>
<tr>
<td>re.M re.MULTILINE</td>
<td>正则表达式中的^操作符能够将给定字符串的每行当作匹配开始</td>
</tr>
<tr>
<td>re.S re.DOTALL</td>
<td>正则表达式中的 . 操作符能够匹配所有字符，默认匹配除换行外的所有字符</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
<p><img src="/.xyz//.%5Cpic%5C5-1.png" alt="5-1"></p>
<h3 id="re-match"><a href="#re-match" class="headerlink" title="re.match()"></a>re.match()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.<span class="keyword">match</span>(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>从一个字符串的开始位置起匹配正则表达式，返回match对象<ul>
<li>pattern：正则表达式的字符串或原生字符串表示</li>
<li>string：待匹配字符串</li>
<li>flags：正则表达式使用时的控制标记</li>
</ul>
</li>
</ul>
<p><img src="/.xyz//.%5Cpic%5C5-2.png" alt="5-2"></p>
<h3 id="re-findall"><a href="#re-findall" class="headerlink" title="re.findall()"></a>re.findall()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.findall(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>搜索字符串，以列表类型返回全部能匹配的子串<ul>
<li>pattern：正则表达式的字符串或原生字符串表示</li>
<li>string：待匹配字符串</li>
<li>flags：正则表达式使用时的控制标记</li>
</ul>
</li>
</ul>
<p><img src="/.xyz//.%5Cpic%5C5-3.png" alt="5-3"></p>
<h3 id="re-split"><a href="#re-split" class="headerlink" title="re.split()"></a>re.split()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(pattern, string, maxsplit=<span class="number">0</span>, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>将一个字符串按照正则表达式匹配结果进行分割，返回列表类型<ul>
<li>pattern：正则表达式的字符串或原生字符串表示</li>
<li>string：待匹配字符串</li>
<li>maxsplit：最大分割数，剩余部分作为最后一个元素输出</li>
<li>flags：正则表达式使用时的控制标记</li>
</ul>
</li>
</ul>
<p><img src="/.xyz//.%5Cpic%5C5-4.png" alt="5-4"></p>
<h3 id="re-finditer"><a href="#re-finditer" class="headerlink" title="re.finditer()"></a>re.finditer()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.finditer(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是 match 对象<ul>
<li>pattern：正则表达式的字符串或原生字符串表示</li>
<li>string：待匹配字符串</li>
<li>flags：正则表达式使用时的控制标记</li>
</ul>
</li>
</ul>
<p><img src="/.xyz//.%5Cpic%5C5-5.png" alt="5-5"></p>
<h3 id="re-sub"><a href="#re-sub" class="headerlink" title="re.sub()"></a>re.sub()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.sub(pattern, repl, string, count=<span class="number">0</span>, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串<ul>
<li>pattern：正则表达式的字符串或原生字符串表示</li>
<li>repl：替换匹配字符串的字符串</li>
<li>string：待匹配字符串</li>
<li>count：匹配的最大替换次数</li>
<li>flags：正则表达式使用时的控制标记</li>
</ul>
</li>
</ul>
<p><img src="/.xyz//.%5Cpic%5C5-6.png" alt="5-6"></p>
<h2 id="re-compile"><a href="#re-compile" class="headerlink" title="re.compile()"></a>re.compile()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.<span class="built_in">compile</span>(pattern, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>将正则表达式的字符串形式编译成正则表达式对象<ul>
<li>pattern：正则表达式的字符串或原生字符串表示</li>
<li>flags：正则表达式使用时的控制标记</li>
</ul>
</li>
</ul>
<h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><h3 id="函数式用法"><a href="#函数式用法" class="headerlink" title="函数式用法"></a>函数式用法</h3><p>函数式用法，即一次性操作，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rst = re.search(<span class="string">r&#x27;[1-9]\d&#123;5&#125;&#x27;</span>, <span class="string">&#x27;BIT 100081&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="面向对象用法"><a href="#面向对象用法" class="headerlink" title="面向对象用法"></a>面向对象用法</h3><p>面向对象用法，即编译后的多次操作，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pat = re.<span class="built_in">compile</span>(<span class="string">r&#x27;[1-9\d&#123;5&#125;]&#x27;</span>)</span><br><span class="line">rst = pat.search(<span class="string">&#x27;BIT 100081&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="match对象"><a href="#match对象" class="headerlink" title="match对象"></a>match对象</h2><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>主要属性如下：</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.string</td>
<td>待匹配的文本</td>
</tr>
<tr>
<td>.re</td>
<td>匹配时使用的pattern对象（正则表达式）</td>
</tr>
<tr>
<td>.pos</td>
<td>正则表达式搜索文本的开始位置</td>
</tr>
<tr>
<td>.endpos</td>
<td>正则表达式搜索文本的结束位置</td>
</tr>
</tbody></table>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>主要方法如下：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.group(0)</td>
<td>获得匹配后的字符串</td>
</tr>
<tr>
<td>.start()</td>
<td>匹配字符串在原始字符串的开始位置</td>
</tr>
<tr>
<td>.end()</td>
<td>匹配字符串在原始字符串的结束位置</td>
</tr>
<tr>
<td>.span()</td>
<td>返回<code>(.start(), .end())</code></td>
</tr>
</tbody></table>
<h3 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h3><p><img src="/.xyz//.%5Cpic%5C5-7.png" alt="5-7"></p>
<h2 id="贪婪匹配与最小匹配"><a href="#贪婪匹配与最小匹配" class="headerlink" title="贪婪匹配与最小匹配"></a>贪婪匹配与最小匹配</h2><p>Re 库默认采用贪婪匹配，即输出匹配最长的子串。</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>*?</td>
<td>前一个字符0次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td>+?</td>
<td>前一个字符1次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td>??</td>
<td>前一个字符0次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td>{m,n}?</td>
<td>扩展前一个字符m至n次（含n），最小匹配</td>
</tr>
</tbody></table>
<p><img src="/.xyz//.%5Cpic%5C5-8.png" alt="5-8"></p>
<h1 id="实例2：淘宝商品信息定向爬虫"><a href="#实例2：淘宝商品信息定向爬虫" class="headerlink" title="实例2：淘宝商品信息定向爬虫"></a>实例2：淘宝商品信息定向爬虫</h1><h2 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h2><h3 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a>功能描述</h3><ul>
<li>目标：获取淘宝搜索页面的信息，提取其中的商品名称和价格。</li>
<li>理解：<ul>
<li>淘宝的搜索接口</li>
<li>翻页的处理</li>
</ul>
</li>
<li>技术路线：requests、re</li>
</ul>
<h3 id="程序的结构设计-1"><a href="#程序的结构设计-1" class="headerlink" title="程序的结构设计"></a>程序的结构设计</h3><ol>
<li>提交商品搜索请求，循环获取页面。</li>
<li>对于每个页面，提取商品名称和价格信息。</li>
<li>将信息输出到屏幕上。</li>
</ol>
<h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getHTMLText</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">passPage</span>(<span class="params">ilt, html</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        plt = re.findall(<span class="string">r&#x27;\&quot;view_price\&quot;\:\&quot;[\d\.]*\&quot;&#x27;</span>, html)</span><br><span class="line">        tlt = re.findall(<span class="string">r&#x27;\&quot;raw_title\&quot;\:\:.*?\&quot;&#x27;</span>, html)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(plt)):</span><br><span class="line">            price = <span class="built_in">eval</span>(plt[i].split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">1</span>])</span><br><span class="line">            title = <span class="built_in">eval</span>(tlt[i].split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">1</span>])</span><br><span class="line">            ilt.append([price, title])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">printGoodsList</span>(<span class="params">ilt</span>):</span><br><span class="line">    tplt = <span class="string">&quot;&#123;:4&#125;\t&#123;:8&#125;\t&#123;:16&#125;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(tplt.<span class="built_in">format</span>(<span class="string">&quot;序号&quot;</span>, <span class="string">&quot;价格&quot;</span>, <span class="string">&quot;商品名称&quot;</span>))</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> g <span class="keyword">in</span> ilt:</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;tplt&quot;</span>.<span class="built_in">format</span>(count, g[<span class="number">0</span>], g[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    goods = <span class="string">&quot;书包&quot;</span></span><br><span class="line">    depth = <span class="number">2</span></span><br><span class="line">    start_url = <span class="string">&quot;https://s.taobao.com/search?q=&quot;</span> + goods</span><br><span class="line">    infoList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url = start_url + <span class="string">&quot;&amp;s=&quot;</span> + <span class="built_in">str</span>(<span class="number">44</span>*i)</span><br><span class="line">            html = getHTMLText(url)</span><br><span class="line">            parsePage(infolist, html)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    printGoodsList(infoList)</span><br><span class="line"></span><br><span class="line">main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="实例3：股票数据定向爬虫"><a href="#实例3：股票数据定向爬虫" class="headerlink" title="实例3：股票数据定向爬虫"></a>实例3：股票数据定向爬虫</h1><h2 id="功能描述-1"><a href="#功能描述-1" class="headerlink" title="功能描述"></a>功能描述</h2><ul>
<li>目标：获取上交所和深交所所有股票的名称和交易信息</li>
<li>输出：保存到文件中</li>
<li>技术路线：requests-bs4-re</li>
</ul>
<h2 id="候选数据网站的选择"><a href="#候选数据网站的选择" class="headerlink" title="候选数据网站的选择"></a>候选数据网站的选择</h2><ul>
<li>选取原则：股票信息静态存在于HTML页面中，非 js 代码生成，没有 robots 协议限制。</li>
<li>选取方法：浏览器 F12，源代码查看等。</li>
<li>选取心态：不要纠结于某个网站，多找信息源尝试。</li>
</ul>
<h2 id="程序的设计结构"><a href="#程序的设计结构" class="headerlink" title="程序的设计结构"></a>程序的设计结构</h2><ol>
<li>从<strong>东方财富网</strong>获取股票列表</li>
<li>根据股票列表逐个到<strong>百度股票</strong>获取个股信息</li>
<li>将结果存储到文件</li>
</ol>
<h2 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#CrawBaiduStocksA.py</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getHTMLText</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getStockList</span>(<span class="params">lst, stockURL</span>):</span><br><span class="line">    html = getHTMLText(stockURL)</span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">&#x27;html.parser&#x27;</span>) </span><br><span class="line">    a = soup.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            href = i.attrs[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">            lst.append(re.findall(<span class="string">r&quot;[s][hz]\d&#123;6&#125;&quot;</span>, href)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getStockInfo</span>(<span class="params">lst, stockURL, fpath</span>):</span><br><span class="line">    <span class="keyword">for</span> stock <span class="keyword">in</span> lst:</span><br><span class="line">        url = stockURL + stock + <span class="string">&quot;.html&quot;</span></span><br><span class="line">        html = getHTMLText(url)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> html==<span class="string">&quot;&quot;</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            infoDict = &#123;&#125;</span><br><span class="line">            soup = BeautifulSoup(html, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">            stockInfo = soup.find(<span class="string">&#x27;div&#x27;</span>,attrs=&#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;stock-bets&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">            name = stockInfo.find_all(attrs=&#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;bets-name&#x27;</span>&#125;)[<span class="number">0</span>]</span><br><span class="line">            infoDict.update(&#123;<span class="string">&#x27;股票名称&#x27;</span>: name.text.split()[<span class="number">0</span>]&#125;)</span><br><span class="line">            </span><br><span class="line">            keyList = stockInfo.find_all(<span class="string">&#x27;dt&#x27;</span>)</span><br><span class="line">            valueList = stockInfo.find_all(<span class="string">&#x27;dd&#x27;</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(keyList)):</span><br><span class="line">                key = keyList[i].text</span><br><span class="line">                val = valueList[i].text</span><br><span class="line">                infoDict[key] = val</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write( <span class="built_in">str</span>(infoDict) + <span class="string">&#x27;\n&#x27;</span> )</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            traceback.print_exc()</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    stock_list_url = <span class="string">&#x27;https://quote.eastmoney.com/stocklist.html&#x27;</span></span><br><span class="line">    stock_info_url = <span class="string">&#x27;https://gupiao.baidu.com/stock/&#x27;</span></span><br><span class="line">    output_file = <span class="string">&#x27;D:/BaiduStockInfo.txt&#x27;</span></span><br><span class="line">    slist=[]</span><br><span class="line">    getStockList(slist, stock_list_url)</span><br><span class="line">    getStockInfo(slist, stock_info_url, output_file)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h2 id="代码-优化"><a href="#代码-优化" class="headerlink" title="代码(优化)"></a>代码(优化)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#CrawBaiduStocksB.py</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getHTMLText</span>(<span class="params">url, code=<span class="string">&quot;utf-8&quot;</span></span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = code</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getStockList</span>(<span class="params">lst, stockURL</span>):</span><br><span class="line">    html = getHTMLText(stockURL, <span class="string">&quot;GB2312&quot;</span>)</span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">&#x27;html.parser&#x27;</span>) </span><br><span class="line">    a = soup.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            href = i.attrs[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">            lst.append(re.findall(<span class="string">r&quot;[s][hz]\d&#123;6&#125;&quot;</span>, href)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getStockInfo</span>(<span class="params">lst, stockURL, fpath</span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> stock <span class="keyword">in</span> lst:</span><br><span class="line">        url = stockURL + stock + <span class="string">&quot;.html&quot;</span></span><br><span class="line">        html = getHTMLText(url)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> html==<span class="string">&quot;&quot;</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            infoDict = &#123;&#125;</span><br><span class="line">            soup = BeautifulSoup(html, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">            stockInfo = soup.find(<span class="string">&#x27;div&#x27;</span>,attrs=&#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;stock-bets&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">            name = stockInfo.find_all(attrs=&#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;bets-name&#x27;</span>&#125;)[<span class="number">0</span>]</span><br><span class="line">            infoDict.update(&#123;<span class="string">&#x27;股票名称&#x27;</span>: name.text.split()[<span class="number">0</span>]&#125;)</span><br><span class="line">            </span><br><span class="line">            keyList = stockInfo.find_all(<span class="string">&#x27;dt&#x27;</span>)</span><br><span class="line">            valueList = stockInfo.find_all(<span class="string">&#x27;dd&#x27;</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(keyList)):</span><br><span class="line">                key = keyList[i].text</span><br><span class="line">                val = valueList[i].text</span><br><span class="line">                infoDict[key] = val</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write( <span class="built_in">str</span>(infoDict) + <span class="string">&#x27;\n&#x27;</span> )</span><br><span class="line">                count = count + <span class="number">1</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\r当前进度: &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(count*<span class="number">100</span>/<span class="built_in">len</span>(lst)),end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\r当前进度: &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(count*<span class="number">100</span>/<span class="built_in">len</span>(lst)),end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    stock_list_url = <span class="string">&#x27;https://quote.eastmoney.com/stocklist.html&#x27;</span></span><br><span class="line">    stock_info_url = <span class="string">&#x27;https://gupiao.baidu.com/stock/&#x27;</span></span><br><span class="line">    output_file = <span class="string">&#x27;D:/BaiduStockInfo.txt&#x27;</span></span><br><span class="line">    slist=[]</span><br><span class="line">    getStockList(slist, stock_list_url)</span><br><span class="line">    getStockInfo(slist, stock_info_url, output_file)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h1 id="Scrapy库"><a href="#Scrapy库" class="headerlink" title="Scrapy库"></a>Scrapy库</h1><h2 id="爬虫框架"><a href="#爬虫框架" class="headerlink" title="爬虫框架"></a>爬虫框架</h2><h3 id="什么是爬虫框架？"><a href="#什么是爬虫框架？" class="headerlink" title="什么是爬虫框架？"></a>什么是爬虫框架？</h3><ul>
<li>爬虫框架是实现网络爬虫功能的一个软件结构和功能组件集合。</li>
<li>爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫。</li>
</ul>
<h3 id="Scrapy爬虫框架"><a href="#Scrapy爬虫框架" class="headerlink" title="Scrapy爬虫框架"></a>Scrapy爬虫框架</h3><h4 id="框架图"><a href="#框架图" class="headerlink" title="框架图"></a>框架图</h4><p><img src="/.xyz//.%5Cpic%5C10-1.png" alt="10-1"></p>
<h4 id="Engine"><a href="#Engine" class="headerlink" title="Engine"></a>Engine</h4><ul>
<li>根据所有模块之间的数据流</li>
<li>根据条件触发事件</li>
<li>不需要用户修改</li>
</ul>
<h4 id="Downloader"><a href="#Downloader" class="headerlink" title="Downloader"></a>Downloader</h4><ul>
<li>根据请求下载网页</li>
<li>不需要用户修改</li>
</ul>
<h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><ul>
<li>对所有爬取请求进行调度管理</li>
<li>不需要用户修改</li>
</ul>
<h4 id="Downloader-Middleware"><a href="#Downloader-Middleware" class="headerlink" title="Downloader Middleware"></a>Downloader Middleware</h4><ul>
<li>目的：实施 Engine、Scheduler和Downloader 之间进行用户可配置的控制。</li>
<li>功能：修改、丢弃、新增请求或响应</li>
<li>用户可以编写配置代码</li>
</ul>
<h4 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h4><ul>
<li>解析 Downloader 返回的响应(Response)</li>
<li>产生爬取项(scraped item)</li>
<li>产生额外的爬取请求(Request)</li>
<li>需要用户编写配置代码</li>
</ul>
<h4 id="Item-Piplines"><a href="#Item-Piplines" class="headerlink" title="Item Piplines"></a>Item Piplines</h4><ul>
<li>以流水线方式处理 Spider 产生的爬取项</li>
<li>由一组操作顺序组成，类似流水线，每个操作是一个 Item Pipline 类型</li>
<li>可能操作包括：清理、检验和查重爬取项中的 HTML 数据、将数据存储到数据库</li>
<li>需要用户编写配置代码</li>
</ul>
<h2 id="与requests库的异同"><a href="#与requests库的异同" class="headerlink" title="与requests库的异同"></a>与requests库的异同</h2><h3 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h3><ul>
<li>两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线。</li>
<li>两者可用性都好，文档丰富，入门简单。</li>
<li>两者都没有处理js、提交表单、应对验证码等功能（可扩展）。</li>
</ul>
<h3 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h3><table>
<thead>
<tr>
<th align="left">requests</th>
<th>Scrapy</th>
</tr>
</thead>
<tbody><tr>
<td align="left">页面级爬虫</td>
<td>网站级爬虫</td>
</tr>
<tr>
<td align="left">功能库</td>
<td>框架</td>
</tr>
<tr>
<td align="left">并发性考虑不足，性能较差</td>
<td>并发性好，性能较高</td>
</tr>
<tr>
<td align="left">重点在于页面下载</td>
<td>重点在于爬虫结构</td>
</tr>
<tr>
<td align="left">定制灵活</td>
<td>一般定制灵活，深度定制困难</td>
</tr>
<tr>
<td align="left">上手十分简单</td>
<td>入门稍难</td>
</tr>
</tbody></table>
<h3 id="如何选择"><a href="#如何选择" class="headerlink" title="如何选择"></a>如何选择</h3><ul>
<li>非常小的需求，requests 库。</li>
<li>不太小的需求，Scrapy 框架。</li>
<li>定制程度很高的需求（不考虑规模），自搭框架，requests &gt; Scrapy。</li>
</ul>
<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><h3 id="Scrapy命令行"><a href="#Scrapy命令行" class="headerlink" title="Scrapy命令行"></a>Scrapy命令行</h3><p>Scrapy 是为持续运行设计的专业爬虫框架，提供操作的 Scrapy 命令行。</p>
<p>在 Linux 下使用以下命令进入 Scrapy 命令行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy -h</span><br></pre></td></tr></table></figure>

<p>Scrapy 命令行格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy &lt;command&gt; [options] [args]</span><br></pre></td></tr></table></figure>

<p>常用命令如下：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>格式</th>
</tr>
</thead>
<tbody><tr>
<td>startproject</td>
<td>创建一个新工程</td>
<td>scrapy startproject &lt;name&gt; [dir]</td>
</tr>
<tr>
<td>genspider</td>
<td>创建一个爬虫</td>
<td>scrapy genspider [options] &lt;name&gt; &lt;domain&gt;</td>
</tr>
<tr>
<td>settings</td>
<td>获得爬虫配置信息</td>
<td>scrapy settings [options]</td>
</tr>
<tr>
<td>crawl</td>
<td>运行一个爬虫</td>
<td>scrapy crawl &lt;spider&gt;</td>
</tr>
<tr>
<td>list</td>
<td>列出工程中所有爬虫</td>
<td>scrapy list</td>
</tr>
<tr>
<td>shell</td>
<td>启动URL调试命令行</td>
<td>scrapy shell [url]</td>
</tr>
</tbody></table>
<h2 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h2><ol>
<li>创建一个工程和 Spider 模板</li>
<li>编写 Spider</li>
<li>编写 Item Pipeline</li>
<li>优化配置策略</li>
</ol>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="Requests-类"><a href="#Requests-类" class="headerlink" title="Requests 类"></a>Requests 类</h3><p>class.scrapy.http.Request()</p>
<ul>
<li>Request 对象表示一个 HTTP 请求。</li>
<li>由 Spider 生成，由 Download 执行。</li>
</ul>
<table>
<thead>
<tr>
<th>属性或方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.url</td>
<td>Request 对应的请求 URL 地址</td>
</tr>
<tr>
<td>.method</td>
<td>对应的请求方法，’GET’ ‘POST’ 等</td>
</tr>
<tr>
<td>.headers</td>
<td>字典类型风格的请求头</td>
</tr>
<tr>
<td>.body</td>
<td>请求内容主体，字符串类型</td>
</tr>
<tr>
<td>.meta</td>
<td>用户添加的扩展信息，在 Scrapy 内部模块间传递信息使用</td>
</tr>
<tr>
<td>.copy()</td>
<td>复制该请求</td>
</tr>
</tbody></table>
<h3 id="Response-类"><a href="#Response-类" class="headerlink" title="Response 类"></a>Response 类</h3><p>class.scrapy.http.Response()</p>
<ul>
<li>Response 对象表示一个 HTTP 响应。</li>
<li>由 Download 生成，由 Spider 处理。</li>
</ul>
<table>
<thead>
<tr>
<th>属性或方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.url</td>
<td>Response 对应的 URL 地址</td>
</tr>
<tr>
<td>.status</td>
<td>HTTP 状态码，默认是 200</td>
</tr>
<tr>
<td>.headers</td>
<td>Response 对应的头部信息</td>
</tr>
<tr>
<td>.body</td>
<td>Response 对应的内容信息，字符串类型</td>
</tr>
<tr>
<td>.flags</td>
<td>一组标记</td>
</tr>
<tr>
<td>.request</td>
<td>产生 Response 类型对应的 Request 对象</td>
</tr>
<tr>
<td>.copy()</td>
<td>复制该响应</td>
</tr>
</tbody></table>
<h3 id="Item-类"><a href="#Item-类" class="headerlink" title="Item 类"></a>Item 类</h3><p>class.scrapy.item.Item()</p>
<ul>
<li>Item 对象表示一个从 HTML 页面中提取的信息内容。</li>
<li>由 Spider 生成，由 Item Pipeline 处理。</li>
<li>Item 类似字典类型，可以按照字典类型操作。</li>
</ul>
<h2 id="Scrapy-爬虫提取信息的方法"><a href="#Scrapy-爬虫提取信息的方法" class="headerlink" title="Scrapy 爬虫提取信息的方法"></a>Scrapy 爬虫提取信息的方法</h2><ul>
<li><p>Beautiful Soup</p>
</li>
<li><p>lxml</p>
</li>
<li><p>re</p>
</li>
<li><p>Path Selector</p>
</li>
<li><p>CSS Selector</p>
<p>使用方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;.css(<span class="string">&#x27;a::attr(href)&#x27;</span>).extract()</span><br></pre></td></tr></table></figure>

<ul>
<li>a：标签名称</li>
<li>href：标签属性</li>
</ul>
</li>
</ul>
<h2 id="实例-3"><a href="#实例-3" class="headerlink" title="实例"></a>实例</h2><ol>
<li><p>建立一个 Scrapy 爬虫工程。</p>
<p>在命令行下输入如下代码创建新 Scrapy 工程。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject python123demo</span><br></pre></td></tr></table></figure>

<p><img src="/.xyz//.%5Cpic%5C11-1.png" alt="11-1"></p>
<p>此时会在文件夹下生成工程目录：</p>
<p><img src="/.xyz//.%5Cpic%5C11-2.png" alt="11-2"></p>
<p>其工程目录结构如下：</p>
<p><img src="/.xyz//.%5Cpic%5C11-3.png" alt="11-3"></p>
<ul>
<li>python123demo&#x2F;  -&gt;  外层目录<ul>
<li>scrapy.cfg  -&gt;  部署 Scrapy 爬虫的配置文件</li>
<li>python123demo&#x2F;  -&gt;  Scrapy 框架的用户自定义 Python 代码<ul>
<li>__init__.py  -&gt;  初始化脚本</li>
<li>items.py  -&gt;  Items 代码模板（继承类）</li>
<li>middlewares.py  -&gt;  Middlewares 代码模板（继承类）</li>
<li>piplines.py  -&gt;  Piplines 代码模板（继承类）</li>
<li>settings.py  -&gt;  Scrapy 爬虫的配置文件</li>
<li>spiders&#x2F;  -&gt; Spiders 代码模板目录（继承类）<ul>
<li>__init__.py  -&gt;  初始文件，无需修改</li>
<li>__pycache__&#x2F;  -&gt;  缓存目录，无需修改</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>在工程中产生一个 Scrapy 爬虫。</p>
<p>使用如下命令生成一个名称为 demo 的爬虫：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider demo python123.io</span><br></pre></td></tr></table></figure>

<p>此时，spiders 文件夹下生成一个 demo.py 文件：</p>
<p><img src="/.xyz//11-4.png" alt="11-4"></p>
<p>文件内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DemoSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;python123.io&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://python123.io/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<ul>
<li>name: 爬虫名字</li>
<li>allowed_domains: 最开始用户提交给命令行的域名</li>
<li>start_urls: scrapy 框架所要爬取页面的初始页面</li>
<li>parse(): 用于响应，解析内容形成字典，发现新的 URL 爬取请求</li>
</ul>
</li>
<li><p>配置产生的 spider 爬虫。</p>
<p>简单版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DemoSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;python123.io&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://python123.io/ws/demo.html&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        fname = response.url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">&#x27;Saved file %s.&#x27;</span> % name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>完整版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DemoSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        urls = [</span><br><span class="line">                <span class="string">&#x27;http://python123.io/ws/demo.html&#x27;</span></span><br><span class="line">                ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=urls, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        fname = response.url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">&#x27;Saved file %s.&#x27;</span> % name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>运行爬虫，获取网页。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl demo</span><br></pre></td></tr></table></figure>

<p>文件夹中出现 demo.html 文件：</p>
<p><img src="/.xyz//11-5.png" alt="11-5"></p>
</li>
</ol>
<h2 id="配置并发连接选项"><a href="#配置并发连接选项" class="headerlink" title="配置并发连接选项"></a>配置并发连接选项</h2><h3 id="settings-py-文件"><a href="#settings-py-文件" class="headerlink" title="settings.py 文件"></a>settings.py 文件</h3><table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>CONCURRENT_REQUESTS</td>
<td>Downloader 最大并发请求下载数量，默认 32</td>
</tr>
<tr>
<td>CONCURRENT_ITEMS</td>
<td>Item Pipeline 最大并发 ITEM 处理数量，默认 100</td>
</tr>
<tr>
<td>CONCURRENT_REQUESTS_PER_DOMAIN</td>
<td>每个目标域名最大的并发请求数量，默认 8</td>
</tr>
<tr>
<td>CONCURRENT_REQUESTS_PER_IP</td>
<td>每个目标 IP 最大的并发请求数量，默认 0，非 0 有效</td>
</tr>
</tbody></table>
<h1 id="实例4：股票数据scrapy爬虫"><a href="#实例4：股票数据scrapy爬虫" class="headerlink" title="实例4：股票数据scrapy爬虫"></a>实例4：股票数据scrapy爬虫</h1><ol>
<li><p>建立工程和 Spider 模板</p>
<p><img src="/.xyz//12-1.png" alt="12-1"></p>
</li>
<li><p>编写 Spider</p>
<ul>
<li>配置 stocks.py 文件</li>
<li>修改对返回页面的处理</li>
<li>修改对新增 URL 爬取请求的处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StocksSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;stocks&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;baidu.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://baidu.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">&#x27;a::attr(href)&#x27;</span>).extract():</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                stock = re.findall(<span class="string">r&quot;[s][hz]\d&#123;6&#125;&quot;</span>, href)[<span class="number">0</span>]</span><br><span class="line">                url = <span class="string">&#x27;https://gupiao.baidu.com/stock/&#x27;</span> + stock + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(url=href, callback=self.parse_stock)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_stock</span>(<span class="params">self, response</span>):</span><br><span class="line">        infoDict = &#123;&#125;</span><br><span class="line">        stockInfo = response.css(<span class="string">&#x27;.stock-bets&#x27;</span>)</span><br><span class="line">        name = stockInfo.css(<span class="string">&#x27;.bets-name&#x27;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        keyList = stockInfo.css(<span class="string">&#x27;dt&#x27;</span>).extract()</span><br><span class="line">        valueList = stockInfo.css(<span class="string">&#x27;dd&#x27;</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(keyList)):</span><br><span class="line">            key = re.findall(<span class="string">r&#x27;&gt;.*&lt;/dt&gt;&#x27;</span>, keyList[i])[<span class="number">0</span>][<span class="number">1</span>:-<span class="number">5</span>]</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                val = re.findall(<span class="string">r&#x27;\d+\.?.*&lt;/dd&gt;&#x27;</span>, valueList[i])[<span class="number">0</span>][<span class="number">0</span>:-<span class="number">5</span>]</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                val = <span class="string">&#x27;--&#x27;</span></span><br><span class="line">            infoDict[key] = val</span><br><span class="line"></span><br><span class="line">        infoDict.update(</span><br><span class="line">            &#123;<span class="string">&#x27;股票名称&#x27;</span>: re.findall(<span class="string">&#x27;\s.*\(&#x27;</span>, name)[<span class="number">0</span>].split()[<span class="number">0</span>] + \</span><br><span class="line">             re.findall(<span class="string">&#x27;\&gt;.*\&lt;&#x27;</span>, name)[<span class="number">0</span>][<span class="number">1</span>:-<span class="number">1</span>]&#125;)</span><br><span class="line">        <span class="keyword">yield</span> infoDict</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写 Pipelines</p>
<ul>
<li><p>配置 pipelines.py 文件</p>
</li>
<li><p>定义对爬取项(Scraped Item)的处理类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> re <span class="keyword">import</span> L</span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaidustocksPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduStocksInfoPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.f = <span class="built_in">open</span>(<span class="string">&#x27;BaiduStocksInfo.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.f.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            line = <span class="built_in">str</span>(<span class="built_in">dict</span>(item)) + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">            self.f.write(line)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 ITEM_PIPELINES 选项</p>
<p><img src="/.xyz//12-2.png" alt="12-2"></p>
</li>
</ul>
</li>
<li><p>启动爬虫</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl stocks</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="yield-关键字"><a href="#yield-关键字" class="headerlink" title="yield 关键字"></a>yield 关键字</h2><h3 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h3><p>yield 关键字 &lt;-&gt; 生成器</p>
<ul>
<li>生成器是一个不断产生值的函数</li>
<li>包含 yield 语句的函数是一个生成器</li>
<li>生成器每次产生一个值（yield 语句），函数被冻结，被唤醒后再产生一个值</li>
</ul>
<h3 id="实例-4"><a href="#实例-4" class="headerlink" title="实例"></a>实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gen</span>(<span class="params">n</span>):</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">		<span class="keyword">yield</span> i ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> gen(<span class="number">5</span>):</span><br><span class="line">	<span class="built_in">print</span>(i, <span class="string">&quot; &quot;</span>, end=<span class="string">&quot;0-1&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/.xyz//0-1.png" alt="0-1"></p>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li>更节省存储空间</li>
<li>响应更迅速</li>
<li>使用更灵活</li>
</ul>

    </div>

    
    
    

      <div>
        
          <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------您可以点击上方Github图标关注我-------------</div>
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------进入<a href="/about/" style="text-align:center;color: #ccc;font-size:14px;">About</a>页面了解更多关于BugMaker的故事-------------</div>
    
</div>

        
      </div>

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Regular-Expression/" rel="tag"># Regular Expression</a>
              <a href="/tags/%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6MOOC/" rel="tag"># 中国大学MOOC</a>
              <a href="/tags/Open-Course/" rel="tag"># Open Course</a>
              <a href="/tags/Network/" rel="tag"># Network</a>
              <a href="/tags/Crawler/" rel="tag"># Crawler</a>
              <a href="/tags/Requests/" rel="tag"># Requests</a>
              <a href="/tags/Beautiful-Soup/" rel="tag"># Beautiful Soup</a>
              <a href="/tags/Scrapy/" rel="tag"># Scrapy</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/02/04/LeetCode-1725-Number%20Of%20Rectangle%20That%20Can%20Form%20The%20Largest%20Square/" rel="prev" title="LeetCode-1725-Number Of Rectangle That Can Form The Largest Square">
      <i class="fa fa-chevron-left"></i> LeetCode-1725-Number Of Rectangle That Can Form The Largest Square
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/02/28/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91uni-app%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8-meHaoTian/" rel="next" title="【笔记】uni-app快速入门-meHaoTian">
      【笔记】uni-app快速入门-meHaoTian <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Preface"><span class="nav-number">1.</span> <span class="nav-text">Preface</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.</span> <span class="nav-text">课程简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8Python-IDE%E5%B7%A5%E5%85%B7"><span class="nav-number">1.2.</span> <span class="nav-text">常用Python IDE工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E7%B1%BB"><span class="nav-number">1.2.1.</span> <span class="nav-text">文本类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%B7%A5%E5%85%B7%E7%B1%BBIDE"><span class="nav-number">1.2.2.</span> <span class="nav-text">集成工具类IDE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">1.3.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Requests%E5%BA%93"><span class="nav-number">2.</span> <span class="nav-text">Requests库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">2.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HTTP-%E5%8D%8F%E8%AE%AE"><span class="nav-number">2.2.</span> <span class="nav-text">HTTP 协议</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#URL"><span class="nav-number">2.2.2.</span> <span class="nav-text">URL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E8%B5%84%E6%BA%90%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">2.2.3.</span> <span class="nav-text">对资源的操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E7%A8%8B"><span class="nav-number">2.2.4.</span> <span class="nav-text">流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PATCH-%E4%B8%8E-PUT-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">2.2.5.</span> <span class="nav-text">PATCH 与 PUT 的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%93"><span class="nav-number">2.3.</span> <span class="nav-text">库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#request"><span class="nav-number">2.3.1.</span> <span class="nav-text">request( )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#get"><span class="nav-number">2.3.2.</span> <span class="nav-text">get( )</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">使用方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Response-%E5%AF%B9%E8%B1%A1"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">Response 对象</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#head"><span class="nav-number">2.3.3.</span> <span class="nav-text">head( )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#post"><span class="nav-number">2.3.4.</span> <span class="nav-text">post( )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#put"><span class="nav-number">2.3.5.</span> <span class="nav-text">put( )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#delete"><span class="nav-number">2.3.6.</span> <span class="nav-text">delete( )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#r-request-headers"><span class="nav-number">2.3.7.</span> <span class="nav-text">r.request.headers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8"><span class="nav-number">2.3.8.</span> <span class="nav-text">异常</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E7%9A%84%E7%88%AC%E5%8F%96"><span class="nav-number">2.4.</span> <span class="nav-text">图片的爬取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E7%9A%84%E9%80%9A%E7%94%A8%E4%BB%A3%E7%A0%81%E6%A1%86%E6%9E%B6"><span class="nav-number">2.5.</span> <span class="nav-text">爬取网页的通用代码框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">2.6.</span> <span class="nav-text">实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B1%EF%BC%9A%E4%BA%AC%E4%B8%9C%E5%95%86%E5%93%81%E9%A1%B5%E9%9D%A2%E7%9A%84%E7%88%AC%E5%8F%96"><span class="nav-number">2.6.1.</span> <span class="nav-text">实例1：京东商品页面的爬取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B2%EF%BC%9A%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%95%86%E5%93%81%E9%A1%B5%E9%9D%A2%E7%9A%84%E7%88%AC%E5%8F%96"><span class="nav-number">2.6.2.</span> <span class="nav-text">实例2：亚马逊商品页面的爬取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B3%EF%BC%9A%E7%99%BE%E5%BA%A6360%E6%90%9C%E7%B4%A2%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E4%BA%A4"><span class="nav-number">2.6.3.</span> <span class="nav-text">实例3：百度360搜索关键词提交</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B4%EF%BC%9A%E7%BD%91%E7%BB%9C%E5%9B%BE%E7%89%87%E7%9A%84%E7%88%AC%E5%8F%96%E4%B8%8E%E5%AD%98%E5%82%A8"><span class="nav-number">2.6.4.</span> <span class="nav-text">实例4：网络图片的爬取与存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B5%EF%BC%9AIP%E5%9C%B0%E5%9D%80%E5%BD%92%E5%B1%9E%E5%9C%B0%E7%9A%84%E8%87%AA%E5%8A%A8%E6%9F%A5%E8%AF%A2"><span class="nav-number">2.6.5.</span> <span class="nav-text">实例5：IP地址归属地的自动查询</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">3.</span> <span class="nav-text">网络爬虫的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E5%B0%BA%E5%AF%B8"><span class="nav-number">3.1.</span> <span class="nav-text">网络爬虫的尺寸</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E9%99%90%E5%88%B6"><span class="nav-number">3.2.</span> <span class="nav-text">网络爬虫的限制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Robots-%E5%8D%8F%E8%AE%AE"><span class="nav-number">3.3.</span> <span class="nav-text">Robots 协议</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-2"><span class="nav-number">3.3.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%B5%E5%AE%88%E6%96%B9%E5%BC%8F"><span class="nav-number">3.3.2.</span> <span class="nav-text">遵守方式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BeautifulSoup%E5%BA%93"><span class="nav-number">4.</span> <span class="nav-text">BeautifulSoup库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-3"><span class="nav-number">4.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E5%99%A8"><span class="nav-number">4.2.</span> <span class="nav-text">解析器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0"><span class="nav-number">4.3.</span> <span class="nav-text">基本元素</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%81%8D%E5%8E%86%E6%96%B9%E6%B3%95"><span class="nav-number">4.4.</span> <span class="nav-text">遍历方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%A1%8C%E9%81%8D%E5%8E%86"><span class="nav-number">4.4.1.</span> <span class="nav-text">下行遍历</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8A%E8%A1%8C%E9%81%8D%E5%8E%86"><span class="nav-number">4.4.2.</span> <span class="nav-text">上行遍历</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E8%A1%8C%E9%81%8D%E5%8E%86"><span class="nav-number">4.4.3.</span> <span class="nav-text">平行遍历</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%BC%E5%BC%8F%E5%8C%96"><span class="nav-number">4.5.</span> <span class="nav-text">格式化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E5%AE%B9%E6%9F%A5%E6%89%BE"><span class="nav-number">4.6.</span> <span class="nav-text">内容查找</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#find-all"><span class="nav-number">4.6.1.</span> <span class="nav-text">find_all</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-4"><span class="nav-number">4.6.1.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#name"><span class="nav-number">4.6.1.2.</span> <span class="nav-text">name</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#attrs"><span class="nav-number">4.6.1.3.</span> <span class="nav-text">attrs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#recursive"><span class="nav-number">4.6.1.4.</span> <span class="nav-text">recursive</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#string"><span class="nav-number">4.6.1.5.</span> <span class="nav-text">string</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%E6%96%B9%E6%B3%95"><span class="nav-number">4.6.2.</span> <span class="nav-text">扩展方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B1%EF%BC%9A%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E6%8E%92%E5%90%8D%E5%AE%9A%E5%90%91%E7%88%AC%E8%99%AB"><span class="nav-number">5.</span> <span class="nav-text">实例1：中国大学排名定向爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="nav-number">5.1.</span> <span class="nav-text">程序的结构设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">5.2.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Re%E5%BA%93"><span class="nav-number">6.</span> <span class="nav-text">Re库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-number">6.1.</span> <span class="nav-text">正则表达式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-5"><span class="nav-number">6.1.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E7%94%A8%E9%80%94"><span class="nav-number">6.1.2.</span> <span class="nav-text">常见用途</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8"><span class="nav-number">6.1.3.</span> <span class="nav-text">使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E6%B3%95"><span class="nav-number">6.1.4.</span> <span class="nav-text">语法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-6"><span class="nav-number">6.1.4.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E7%AC%A6"><span class="nav-number">6.1.4.2.</span> <span class="nav-text">常用操作符</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B-1"><span class="nav-number">6.1.4.3.</span> <span class="nav-text">实例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Re%E5%BA%93%E6%A6%82%E8%BF%B0"><span class="nav-number">6.2.</span> <span class="nav-text">Re库概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD%E5%87%BD%E6%95%B0"><span class="nav-number">6.3.</span> <span class="nav-text">主要功能函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%87%E6%80%BB"><span class="nav-number">6.3.1.</span> <span class="nav-text">汇总</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#re-search"><span class="nav-number">6.3.2.</span> <span class="nav-text">re.search()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#re-match"><span class="nav-number">6.3.3.</span> <span class="nav-text">re.match()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#re-findall"><span class="nav-number">6.3.4.</span> <span class="nav-text">re.findall()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#re-split"><span class="nav-number">6.3.5.</span> <span class="nav-text">re.split()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#re-finditer"><span class="nav-number">6.3.6.</span> <span class="nav-text">re.finditer()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#re-sub"><span class="nav-number">6.3.7.</span> <span class="nav-text">re.sub()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#re-compile"><span class="nav-number">6.4.</span> <span class="nav-text">re.compile()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8%E6%B3%95"><span class="nav-number">6.5.</span> <span class="nav-text">用法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%BC%8F%E7%94%A8%E6%B3%95"><span class="nav-number">6.5.1.</span> <span class="nav-text">函数式用法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%94%A8%E6%B3%95"><span class="nav-number">6.5.2.</span> <span class="nav-text">面向对象用法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#match%E5%AF%B9%E8%B1%A1"><span class="nav-number">6.6.</span> <span class="nav-text">match对象</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%9E%E6%80%A7"><span class="nav-number">6.6.1.</span> <span class="nav-text">属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">6.6.2.</span> <span class="nav-text">方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B-2"><span class="nav-number">6.6.3.</span> <span class="nav-text">实例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%AA%E5%A9%AA%E5%8C%B9%E9%85%8D%E4%B8%8E%E6%9C%80%E5%B0%8F%E5%8C%B9%E9%85%8D"><span class="nav-number">6.7.</span> <span class="nav-text">贪婪匹配与最小匹配</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B2%EF%BC%9A%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%E5%AE%9A%E5%90%91%E7%88%AC%E8%99%AB"><span class="nav-number">7.</span> <span class="nav-text">实例2：淘宝商品信息定向爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-7"><span class="nav-number">7.1.</span> <span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%9F%E8%83%BD%E6%8F%8F%E8%BF%B0"><span class="nav-number">7.1.1.</span> <span class="nav-text">功能描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1-1"><span class="nav-number">7.1.2.</span> <span class="nav-text">程序的结构设计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="nav-number">7.2.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B3%EF%BC%9A%E8%82%A1%E7%A5%A8%E6%95%B0%E6%8D%AE%E5%AE%9A%E5%90%91%E7%88%AC%E8%99%AB"><span class="nav-number">8.</span> <span class="nav-text">实例3：股票数据定向爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%9F%E8%83%BD%E6%8F%8F%E8%BF%B0-1"><span class="nav-number">8.1.</span> <span class="nav-text">功能描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%80%99%E9%80%89%E6%95%B0%E6%8D%AE%E7%BD%91%E7%AB%99%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">8.2.</span> <span class="nav-text">候选数据网站的选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%BB%93%E6%9E%84"><span class="nav-number">8.3.</span> <span class="nav-text">程序的设计结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="nav-number">8.4.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-%E4%BC%98%E5%8C%96"><span class="nav-number">8.5.</span> <span class="nav-text">代码(优化)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy%E5%BA%93"><span class="nav-number">9.</span> <span class="nav-text">Scrapy库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6"><span class="nav-number">9.1.</span> <span class="nav-text">爬虫框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%9F"><span class="nav-number">9.1.1.</span> <span class="nav-text">什么是爬虫框架？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6"><span class="nav-number">9.1.2.</span> <span class="nav-text">Scrapy爬虫框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6%E5%9B%BE"><span class="nav-number">9.1.2.1.</span> <span class="nav-text">框架图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Engine"><span class="nav-number">9.1.2.2.</span> <span class="nav-text">Engine</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Downloader"><span class="nav-number">9.1.2.3.</span> <span class="nav-text">Downloader</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Scheduler"><span class="nav-number">9.1.2.4.</span> <span class="nav-text">Scheduler</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Downloader-Middleware"><span class="nav-number">9.1.2.5.</span> <span class="nav-text">Downloader Middleware</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spider"><span class="nav-number">9.1.2.6.</span> <span class="nav-text">Spider</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Item-Piplines"><span class="nav-number">9.1.2.7.</span> <span class="nav-text">Item Piplines</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8Erequests%E5%BA%93%E7%9A%84%E5%BC%82%E5%90%8C"><span class="nav-number">9.2.</span> <span class="nav-text">与requests库的异同</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%90%8C%E7%82%B9"><span class="nav-number">9.2.1.</span> <span class="nav-text">相同点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E7%82%B9"><span class="nav-number">9.2.2.</span> <span class="nav-text">不同点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9"><span class="nav-number">9.2.3.</span> <span class="nav-text">如何选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="nav-number">9.3.</span> <span class="nav-text">常用命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E5%91%BD%E4%BB%A4%E8%A1%8C"><span class="nav-number">9.3.1.</span> <span class="nav-text">Scrapy命令行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%AD%A5%E9%AA%A4"><span class="nav-number">9.4.</span> <span class="nav-text">使用步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">9.5.</span> <span class="nav-text">数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Requests-%E7%B1%BB"><span class="nav-number">9.5.1.</span> <span class="nav-text">Requests 类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Response-%E7%B1%BB"><span class="nav-number">9.5.2.</span> <span class="nav-text">Response 类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Item-%E7%B1%BB"><span class="nav-number">9.5.3.</span> <span class="nav-text">Item 类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%E4%BF%A1%E6%81%AF%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">9.6.</span> <span class="nav-text">Scrapy 爬虫提取信息的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B-3"><span class="nav-number">9.7.</span> <span class="nav-text">实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%B9%B6%E5%8F%91%E8%BF%9E%E6%8E%A5%E9%80%89%E9%A1%B9"><span class="nav-number">9.8.</span> <span class="nav-text">配置并发连接选项</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#settings-py-%E6%96%87%E4%BB%B6"><span class="nav-number">9.8.1.</span> <span class="nav-text">settings.py 文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B4%EF%BC%9A%E8%82%A1%E7%A5%A8%E6%95%B0%E6%8D%AEscrapy%E7%88%AC%E8%99%AB"><span class="nav-number">10.</span> <span class="nav-text">实例4：股票数据scrapy爬虫</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">11.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#yield-%E5%85%B3%E9%94%AE%E5%AD%97"><span class="nav-number">11.1.</span> <span class="nav-text">yield 关键字</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-8"><span class="nav-number">11.1.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B-4"><span class="nav-number">11.1.2.</span> <span class="nav-text">实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8A%BF"><span class="nav-number">11.1.3.</span> <span class="nav-text">优势</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="BugMaker"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">BugMaker</p>
  <div class="site-description" itemprop="description"></div>
</div>

<div id="music163player">
  <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=280 height=86 src="//music.163.com/outchain/player?type=2&id=167595&auto=1&height=66">
  </iframe>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">97</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">96</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/BugMakerH/" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;BugMakerH&#x2F;" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/hhyi2008@icloud.com" title="E-Mail → hhyi2008@icloud.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备2021039738号 </a>
  </div>

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">BugMaker</span>
</div>


<span id="sitetime"></span>
<script language=javascript>
	function siteTime(){
		window.setTimeout("siteTime()", 1000);
		var seconds = 1000;
		var minutes = seconds * 60;
		var hours = minutes * 60;
		var days = hours * 24;
		var years = days * 365;
		var today = new Date();
		var todayYear = today.getFullYear();
		var todayMonth = today.getMonth()+1;
		var todayDate = today.getDate();
		var todayHour = today.getHours();
		var todayMinute = today.getMinutes();
		var todaySecond = today.getSeconds();
		/* 
		Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
		year - 作为date对象的年份，为4位年份值
		month - 0-11之间的整数，做为date对象的月份
		day - 1-31之间的整数，做为date对象的天数
		hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
		minutes - 0-59之间的整数，做为date对象的分钟数
		seconds - 0-59之间的整数，做为date对象的秒数
		microseconds - 0-999之间的整数，做为date对象的毫秒数
        */
		var t1 = Date.UTC(2021,12,13,02,59,34); //北京时间2021-12-12 11:59:34
		var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
		var diff = t2-t1;
		var diffYears = Math.floor(diff/years);
		var diffDays = Math.floor((diff/days)-diffYears*365);
		var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
		var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
		var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
		document.getElementById("sitetime").innerHTML=" This site has been running safely for "+/*diffYears+" 年 "+*/diffDays+" days "+diffHours+" hours "+diffMinutes+" minutes "+diffSeconds+" seconds.";
	}
	siteTime();
</script><br>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>


<!--
<script>
    //   自定义邮箱审核规则
    document.body.addEventListener('click', function(e) {
        if (e.target.classList.contains('vsubmit')) {
            const email = document.querySelector('input[type=email]');
            const nick = document.querySelector('input[name=nick]');
            const reg = /^[A-Za-z0-9\u4e00-\u9fa5]+@[a-zA-Z0-9_-]+(\.[a-zA-Z0-9_-]+)+$/;
            if (!email.value || !nick.value || !reg.test(email.value)) {
                const str = '<div class="valert text-center"><div class="vtext">请填写正确的昵称和邮箱！</div></div>';
                const vmark = document.querySelector('.vmark');
                vmark.innerHTML = str;
                vmark.style.display = 'block';
                setTimeout(function() {
                    vmark.style.display = 'none';
                    vmark.innerHTML = '';
                }, 2500);
            }
        }
    })
</script>
-->

<!--
<script>
      var now = new Date(); 
      function createtime() { 
          var grt= new Date("12/12/2021 11:59:34");//此处修改你的建站时间或者网站上线时间 
          now.setTime(now.getTime()+250); 
          days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
          hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
          if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
          mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
          seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
          snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
          document.getElementById("timeDate").innerHTML = "This site has been running safely for "+dnum+" days. "; 
          document.getElementById("times").innerHTML = hnum + " hours " + mnum + " minutes " + snum + " seconds"; 
      } 
  setInterval("createtime()",250);
  </script>
  -->
        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
